{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (4.45.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\g00093988\\appdata\\local\\anaconda3\\envs\\neural\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\g00093988\\.cache\\huggingface\\hub\\models--aubmindlab--bert-base-arabertv2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the tokenizer and model for AraBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
    "araber_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "data['Metaphorical Density'] = pd.to_numeric(data['Metaphorical Density'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_verses(verses, tokenizer, max_length):\n",
    "    encoded = tokenizer(\n",
    "        verses.tolist(),  # Convert series to list\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "# Example preprocessing\n",
    "input_ids, attention_mask = preprocess_verses(data['البيت'], tokenizer, max_length=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assume data is already loaded\n",
    "features_to_encode = ['العصر', 'الشاعر', 'الديوان', 'القافية', 'البحر', 'Semantic']\n",
    "\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "one_hot_encoded = encoder.fit_transform(data[features_to_encode])\n",
    "\n",
    "# Create a DataFrame with the encoded columns\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, \n",
    "                          columns=encoder.get_feature_names_out(features_to_encode))\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "y = pd.concat([one_hot_df,data['Metaphorical Density']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_cols = [\n",
    "    \"Semantic_acceptance\", \"Semantic_admiration\", \"Semantic_ambiguity\", \"Semantic_anger\", \"Semantic_anguish\", \n",
    "    \"Semantic_anticipation\", \"Semantic_anxiety\", \"Semantic_awe\", \"Semantic_bittersweet\", \"Semantic_calm\", \n",
    "    \"Semantic_caution\", \"Semantic_concern\", \"Semantic_confidence\", \"Semantic_conflict\", \"Semantic_confusion\", \n",
    "    \"Semantic_contempt\", \"Semantic_contentment\", \"Semantic_curiosity\", \"Semantic_defiance\", \"Semantic_desire\", \n",
    "    \"Semantic_despair\", \"Semantic_determination\", \"Semantic_disgust\", \"Semantic_fear\", \"Semantic_frustration\", \n",
    "    \"Semantic_gratitude\", \"Semantic_hope\", \"Semantic_joy\", \"Semantic_longing\", \"Semantic_love\", \"Semantic_melancholy\", \n",
    "    \"Semantic_neutral\", \"Semantic_nostalgia\", \"Semantic_optimism\", \"Semantic_pessimism\", \"Semantic_pride\", \"Semantic_regret\", \n",
    "    \"Semantic_revenge\", \"Semantic_romantic\", \"Semantic_sadness\", \"Semantic_sorrow\", \"Semantic_surprise\", \"Semantic_trust\", \n",
    "    \"Semantic_trusted\", \"Semantic_urgency\", \"Semantic_warning\", \"Semantic_wonder\"\n",
    "]\n",
    "\n",
    "era_cols = [\n",
    "    \"العصر_المخضرمين\", \"العصر_قبل الإسلام\",\"العصر_الأموي\", \"العصر_العباسي\"\n",
    "]\n",
    "\n",
    "density_col = [\n",
    "    \"Metaphorical Density\"\n",
    "]\n",
    "\n",
    "asr = y[era_cols]\n",
    "semantic = y[semantic_cols]\n",
    "density = y[density_col]\n",
    "\n",
    "num_classes_asr = len(set(data['العصر']))\n",
    "num_classes_semantic = len(set(data['Semantic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ArabicPoetryDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, y_asr, y_semantic, y_density):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.y_asr = y_asr\n",
    "        self.y_semantic = y_semantic\n",
    "        self.y_density = y_density\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.input_ids[idx],\n",
    "            self.attention_mask[idx],\n",
    "            self.y_asr[idx],\n",
    "            self.y_semantic[idx],\n",
    "            self.y_density[idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert one-hot encoded columns and density to tensors\n",
    "y_asr = torch.tensor(asr.values, dtype=torch.float32)  # One-hot encoded ASR\n",
    "y_semantic = torch.tensor(semantic.values, dtype=torch.float32)  # One-hot encoded Semantic\n",
    "y_density = torch.tensor(density.values, dtype=torch.float32).view(-1, 1)  # Reshape for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ArabicPoetryDataset(input_ids, attention_mask, y_asr, y_semantic, y_density)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class AraBERTMultitaskModel(nn.Module):\n",
    "    def __init__(self, araber_model, num_asr_classes, num_semantic_classes):\n",
    "        super(AraBERTMultitaskModel, self).__init__()\n",
    "        self.araber = araber_model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Output layers\n",
    "        self.asr_output = nn.Linear(768, num_asr_classes)  # AraBERT hidden size = 768\n",
    "        self.semantic_output = nn.Linear(768, num_semantic_classes)\n",
    "        self.density_output = nn.Linear(768, 1)  # Single output for numerical prediction\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get embeddings from AraBERT\n",
    "        outputs = self.araber(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # CLS token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Multitask outputs\n",
    "        asr_logits = self.asr_output(pooled_output)\n",
    "        semantic_logits = self.semantic_output(pooled_output)\n",
    "        density_value = self.density_output(pooled_output)\n",
    "        \n",
    "        return asr_logits, semantic_logits, density_value\n",
    "\n",
    "# Instantiate the model\n",
    "model = AraBERTMultitaskModel(araber_model, num_asr_classes = num_classes_asr, num_semantic_classes=num_classes_semantic)  # Adjust classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss functions\n",
    "criterion_asr = nn.CrossEntropyLoss()  # For categorical output\n",
    "criterion_semantic = nn.CrossEntropyLoss()  # For categorical output\n",
    "criterion_density = nn.MSELoss()  # For numerical output\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 29\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# print(loss_asr)       \u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# print(loss_density)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# print(loss_semantic) \u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     adam(\n\u001b[0;32m    169\u001b[0m         params_with_grad,\n\u001b[0;32m    170\u001b[0m         grads,\n\u001b[0;32m    171\u001b[0m         exp_avgs,\n\u001b[0;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    174\u001b[0m         state_steps,\n\u001b[0;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m func(params,\n\u001b[0;32m    319\u001b[0m      grads,\n\u001b[0;32m    320\u001b[0m      exp_avgs,\n\u001b[0;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    323\u001b[0m      state_steps,\n\u001b[0;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\g00093988\\AppData\\Local\\anaconda3\\envs\\neural\\Lib\\site-packages\\torch\\optim\\adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, y_asr, y_semantic, y_density = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        asr_logits, semantic_logits, density_value = model(input_ids, attention_mask)\n",
    "        print(asr_logits)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_asr = criterion_asr(asr_logits, y_asr)\n",
    "        loss_semantic = criterion_semantic(semantic_logits, y_semantic)\n",
    "        loss_density = criterion_density(density_value.squeeze(), y_density)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = loss_asr + loss_semantic + loss_density\n",
    "        # print(loss)\n",
    "        # print(loss_asr)       \n",
    "        # print(loss_density)\n",
    "        # print(loss_semantic) \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
